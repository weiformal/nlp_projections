{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e79e54f-1b46-46db-bd7e-563d77a4a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:08:37,161 - modelscope - WARNING - Model revision not specified, use revision: v1.1.9\n",
      "Downloading: 100%|██████████| 8.21k/8.21k [00:00<00:00, 23.9kB/s]\n",
      "Downloading: 100%|██████████| 50.8k/50.8k [00:00<00:00, 151kB/s]\n",
      "Downloading: 100%|██████████| 244k/244k [00:00<00:00, 577kB/s]\n",
      "Downloading: 100%|██████████| 135k/135k [00:00<00:00, 339kB/s]\n",
      "Downloading: 100%|██████████| 910/910 [00:00<00:00, 2.28kB/s]\n",
      "Downloading: 100%|██████████| 77.0/77.0 [00:00<00:00, 240B/s]\n",
      "Downloading: 100%|██████████| 2.29k/2.29k [00:00<00:00, 7.76kB/s]\n",
      "Downloading: 100%|██████████| 1.88k/1.88k [00:00<00:00, 6.46kB/s]\n",
      "Downloading: 100%|██████████| 249/249 [00:00<00:00, 486B/s]\n",
      "Downloading: 100%|██████████| 1.63M/1.63M [00:00<00:00, 3.07MB/s]\n",
      "Downloading: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.24MB/s]\n",
      "Downloading: 100%|██████████| 2.64M/2.64M [00:00<00:00, 4.84MB/s]\n",
      "Downloading: 100%|██████████| 6.73k/6.73k [00:00<00:00, 20.9kB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 192kB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 173kB/s]\n",
      "Downloading: 100%|██████████| 1.83G/1.83G [00:18<00:00, 106MB/s] \n",
      "Downloading: 100%|██████████| 1.88G/1.88G [00:18<00:00, 109MB/s] \n",
      "Downloading: 100%|██████████| 1.88G/1.88G [00:19<00:00, 104MB/s] \n",
      "Downloading: 100%|██████████| 1.88G/1.88G [00:18<00:00, 109MB/s] \n",
      "Downloading: 100%|██████████| 1.88G/1.88G [00:19<00:00, 105MB/s] \n",
      "Downloading: 100%|██████████| 1.88G/1.88G [00:20<00:00, 101MB/s] \n",
      "Downloading: 100%|██████████| 1.88G/1.88G [00:19<00:00, 102MB/s] \n",
      "Downloading: 100%|██████████| 1.24G/1.24G [00:12<00:00, 107MB/s] \n",
      "Downloading: 100%|██████████| 19.1k/19.1k [00:00<00:00, 64.5kB/s]\n",
      "Downloading: 100%|██████████| 54.3k/54.3k [00:00<00:00, 120kB/s]\n",
      "Downloading: 100%|██████████| 15.0k/15.0k [00:00<00:00, 43.3kB/s]\n",
      "Downloading: 100%|██████████| 237k/237k [00:00<00:00, 455kB/s]\n",
      "Downloading: 100%|██████████| 116k/116k [00:00<00:00, 279kB/s]\n",
      "Downloading: 100%|██████████| 2.44M/2.44M [00:00<00:00, 4.09MB/s]\n",
      "Downloading: 100%|██████████| 473k/473k [00:00<00:00, 1.09MB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 46.2kB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 211kB/s]\n",
      "Downloading: 100%|██████████| 46.4k/46.4k [00:00<00:00, 136kB/s]\n",
      "Downloading: 100%|██████████| 0.98M/0.98M [00:00<00:00, 1.55MB/s]\n",
      "Downloading: 100%|██████████| 205k/205k [00:00<00:00, 407kB/s]\n",
      "Downloading: 100%|██████████| 19.4k/19.4k [00:00<00:00, 44.5kB/s]\n",
      "Downloading: 100%|██████████| 11.6k/11.6k [00:00<00:00, 28.9kB/s]\n",
      "Downloading: 100%|██████████| 302k/302k [00:00<00:00, 567kB/s]\n",
      "Downloading: 100%|██████████| 615k/615k [00:00<00:00, 1.26MB/s]\n",
      "Downloading: 100%|██████████| 376k/376k [00:00<00:00, 676kB/s]\n",
      "Downloading: 100%|██████████| 445k/445k [00:00<00:00, 954kB/s]\n",
      "Downloading: 100%|██████████| 33.7k/33.7k [00:00<00:00, 106kB/s]\n",
      "Downloading: 100%|██████████| 395k/395k [00:00<00:00, 728kB/s]\n",
      "Downloading: 100%|██████████| 176k/176k [00:00<00:00, 443kB/s]\n",
      "Downloading: 100%|██████████| 182k/182k [00:00<00:00, 458kB/s]\n",
      "Downloading: 100%|██████████| 824k/824k [00:00<00:00, 1.54MB/s]\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 715kB/s]\n",
      "Downloading: 100%|██████████| 433k/433k [00:00<00:00, 911kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.01MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 878kB/s]\n",
      "Downloading: 100%|██████████| 9.39k/9.39k [00:00<00:00, 10.9kB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 813kB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 203kB/s]\n",
      "Downloading: 100%|██████████| 173/173 [00:00<00:00, 512B/s]\n",
      "Downloading: 100%|██████████| 41.9k/41.9k [00:00<00:00, 115kB/s]\n",
      "Downloading: 100%|██████████| 230k/230k [00:00<00:00, 427kB/s]\n",
      "Downloading: 100%|██████████| 1.27M/1.27M [00:00<00:00, 2.40MB/s]\n",
      "Downloading: 100%|██████████| 664k/664k [00:00<00:00, 1.12MB/s]\n",
      "Downloading: 100%|██████████| 404k/404k [00:00<00:00, 746kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/weibs/.cache/modelscope/hub/qwen/Qwen-7B-chat'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('qwen/Qwen-7B-chat')\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1cc81-c494-47e8-9cc8-8d19611c9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch,einops\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "device = \"cuda:1\"\n",
    "# （2）加载python split_json.py拼接好之后的1000条数据\n",
    "dataset = load_dataset(\"json\",data_files=\"dataset.json\",split=\"train\")\n",
    " \n",
    "# （3）模型配置\n",
    "base_model_name = '/home/weibs/.cache/modelscope/hub/qwen/Qwen-7B-chat' # 路径需要根据模型部署路径修改\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,     #在4bit上，进行量化\n",
    "    bnb_4bit_use_double_quant=True,     # 嵌套量化，每个参数可以多节省0.4位\n",
    "    bnb_4bit_quant_type=\"nf4\",     #NF4（normalized float）或纯FP4量化 博客说推荐NF4\n",
    "    bnb_4bit_compute_dtype=torch.float16)\n",
    " \n",
    "# （4）QloRA微调参数配置\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    " \n",
    "# （5）加载部署好的本地模型（Llama）\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,#本地模型名称\n",
    "    # quantization_config=bnb_config,#上面本地模型的配置\n",
    "    device_map=\"auto\",#使用GPU的编号\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    " \n",
    "# （6）长文本拆分成最小的单元词（即token）\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    " \n",
    " \n",
    "# （7）训练参数配置\n",
    "output_dir = \"./results\"\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=output_dir, #训练后输出目录\n",
    "    per_device_train_batch_size=4, #每个GPU的批处理数据量\n",
    "    gradient_accumulation_steps=4, #在执行反向传播/更新过程之前，要累积其梯度的更新步骤数\n",
    "    learning_rate=2e-4, #超参、初始学习率。太大模型不稳定，太小则模型不能收敛\n",
    "    logging_steps=10, #两个日志记录之间的更新步骤数\n",
    "    max_steps=100 #要执行的训练步骤总数\n",
    ")\n",
    "max_seq_length = 512\n",
    "#TrainingArguments 的参数详解：https://blog.csdn.net/qq_33293040/article/details/117376382\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    " \n",
    "# （8）运行程序，进行微调\n",
    "trainer.train()\n",
    " \n",
    "# （9）保存模型\n",
    "import os\n",
    "output_dir = os.path.join(output_dir, \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db0c26-e410-4c5c-8339-1e18bcf113cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    " \n",
    "#设置原来本地模型的地址\n",
    "model_name_or_path = '/root/autodl-tmp/Llama2-chat-13B-Chinese-50W'\n",
    "#设置微调后模型的地址，就是上面的那个地址\n",
    "adapter_name_or_path = '/root/autodl-tmp/results/final_checkpoint'\n",
    "#设置合并后模型的导出地址\n",
    "save_path = '/root/autodl-tmp/new_model'\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "print(\"load model success\")\n",
    "model = PeftModel.from_pretrained(model, adapter_name_or_path)\n",
    "print(\"load adapter success\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"merge success\")\n",
    " \n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "print(\"save done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfactory",
   "language": "python",
   "name": "llmfactory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
