{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6892e12-4857-4329-adad-ab2b74858645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# langsmith的环境\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95743016-d844-49c6-b8fc-d755b30bba87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI,OpenAI\n",
    "glm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    # model=\"glm-3-turbo\",\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=\"036549c5f123aa1a284cbd502edad88c.OpF6mns4FtQ4MaHO\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d90e7-1bee-4d67-b381-548f669e7536",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from vllm import LLM\n",
    "\n",
    "\n",
    "ollm = ChatOllama(model = \"qwen2\")\n",
    "# llm = VLLM(model=\"/home/weibs/.cache/modelscope/hub/qwen/Qwen-7B-chat\",trust_remote_code=True,enforce_eager=True)\n",
    "# llm.invoke(\"你好\")\n",
    "# from langchain_community.chat_models import ChatZhipuAI\n",
    "# import os\n",
    "# os.environ[\"ZHIPUAI_API_KEY\"] = \"036549c5f123aa1a284cbd502edad88c.OpF6mns4FtQ4MaHO\"\n",
    "# model = ChatZhipuAI(model=\"glm-4\",streamer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9254bc-48fe-4b4f-8b0a-f5214b03f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import requests\n",
    "import json\n",
    "from typing import Any, List, Optional\n",
    "from langchain.llms.base import LLM\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "# from output_parse import getFirstMsg,parse_tool\n",
    " \n",
    " \n",
    "class MyChatGLM(LLM):\n",
    "    max_token: int = 8192\n",
    "    # do_sample: bool = False\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.8\n",
    "    top_p = 0.8\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history: List = []\n",
    "    has_search: bool = False\n",
    "    model_name: str = \"glm4-9b-chat\"\n",
    "    url: str = \"http://0.0.0.0:6667/chat/\"\n",
    "    tools: List = []\n",
    " \n",
    "    # def __init__(self):\n",
    "    #     super().__init__()\n",
    " \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"MyChatGLM\"\n",
    " \n",
    " \n",
    "    def _tool_history(self, prompt: str):\n",
    "            ans = []\n",
    " \n",
    "            tool_prompts = prompt.split(\n",
    "                \"You have access to the following tools:\\n\\n\")[1].split(\"\\n\\nUse a json blob\")[0].split(\"\\n\")\n",
    "            tools_json = []\n",
    " \n",
    "            for tool_desc in tool_prompts:\n",
    "                name = tool_desc.split(\":\")[0]\n",
    "                description = tool_desc.split(\", args:\")[0].split(\":\")[1].strip()\n",
    "                parameters_str = tool_desc.split(\"args:\")[1].strip()\n",
    "                parameters_dict = ast.literal_eval(parameters_str)\n",
    "                params_cleaned = {}\n",
    "                for param, details in parameters_dict.items():\n",
    "                    params_cleaned[param] = {'description': details['description'], 'type': details['type']}\n",
    " \n",
    "                tools_json.append({\n",
    "                    \"name\": name,\n",
    "                    \"description\": description,\n",
    "                    \"parameters\": params_cleaned\n",
    "                })\n",
    " \n",
    "            ans.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Answer the following questions as best as you can. You have access to the following tools:\",\n",
    "                \"tools\": tools_json\n",
    "            })\n",
    " \n",
    "            dialog_parts = prompt.split(\"Human: \")\n",
    "            for part in dialog_parts[1:]:\n",
    "                if \"\\nAI: \" in part:\n",
    "                    user_input, ai_response = part.split(\"\\nAI: \")\n",
    "                    ai_response = ai_response.split(\"\\n\")[0]\n",
    "                else:\n",
    "                    user_input = part\n",
    "                    ai_response = None\n",
    " \n",
    "                ans.append({\"role\": \"user\", \"content\": user_input.strip()})\n",
    "                if ai_response:\n",
    "                    ans.append({\"role\": \"assistant\", \"content\": ai_response.strip()})\n",
    " \n",
    "            query = dialog_parts[-1].split(\"\\n\")[0]\n",
    "            return ans, query\n",
    " \n",
    "    def _extract_observation(self, prompt: str):\n",
    "        return_json = prompt.split(\"Observation: \")[-1].split(\"\\nThought:\")[0]\n",
    "        self.history.append({\n",
    "            \"role\": \"observation\",\n",
    "            \"content\": return_json\n",
    "        })\n",
    "        return\n",
    " \n",
    "    def _extract_tool(self):\n",
    "        if len(self.history[-1][\"metadata\"]) > 0:\n",
    "            metadata = self.history[-1][\"metadata\"]\n",
    "            content = self.history[-1][\"content\"]\n",
    " \n",
    "            lines = content.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'tool_call(' in line and ')' in line and self.has_search is False:\n",
    "                    # 获取括号内的字符串\n",
    "                    params_str = line.split('tool_call(')[-1].split(')')[0]\n",
    " \n",
    "                    # 解析参数对\n",
    "                    params_pairs = [param.split(\"=\") for param in params_str.split(\",\") if \"=\" in param]\n",
    "                    params = {pair[0].strip(): pair[1].strip().strip(\"'\\\"\") for pair in params_pairs}\n",
    "                    action_json = {\n",
    "                        \"action\": metadata,\n",
    "                        \"action_input\": params\n",
    "                    }\n",
    "                    self.has_search = True\n",
    "                    print(\"*****Action*****\")\n",
    "                    print(action_json)\n",
    "                    print(\"*****Answer*****\")\n",
    "                    return f\"\"\"\n",
    "Action: \n",
    "```\n",
    "{json.dumps(action_json, ensure_ascii=False)}\n",
    "```\"\"\"\n",
    "        final_answer_json = {\n",
    "            \"action\": \"Final Answer\",\n",
    "            \"action_input\": self.history[-1][\"content\"]\n",
    "        }\n",
    "        self.has_search = False\n",
    "        return f\"\"\"\n",
    "Action: \n",
    "```\n",
    "{json.dumps(final_answer_json, ensure_ascii=False)}\n",
    "```\"\"\"\n",
    " \n",
    "    def _call(self, prompt: str, history: List = [], stop: Optional[List[str]] = [\"<|user|>\"]):\n",
    "        if not self.has_search:\n",
    "            self.history, query = self._tool_history(prompt)\n",
    "            if self.history[0]:\n",
    "                self.tools = self.history[0][\"tools\"]\n",
    "        else:\n",
    "            self._extract_observation(prompt)\n",
    "            query = \"\"\n",
    "        print(self.history)\n",
    "        data = {}\n",
    "        data[\"model\"] = self.model_name\n",
    "        data[\"messages\"] = self.history\n",
    "        data[\"temperature\"] = self.temperature\n",
    "        data[\"max_tokens\"] = self.max_token\n",
    "        data[\"tools\"] = self.tools\n",
    "        resp = self.doRequest(data)\n",
    "        \n",
    "        msg = {}\n",
    "        respjson = json.loads(resp)\n",
    "        if respjson[\"choices\"]:\n",
    "            if respjson[\"choices\"][0][\"finish_reason\"] == 'function_call':\n",
    "                msg[\"metadata\"] = respjson[\"choices\"][0][\"message\"][\"function_call\"][\"name\"]\n",
    "            else:\n",
    "                msg[\"metadata\"] = ''\n",
    "            msg[\"role\"] = \"assistant\"\n",
    "            msg[\"content\"] = respjson[\"choices\"][0][\"message\"][\"content\"]\n",
    " \n",
    "        self.history.append(msg)\n",
    "        print(self.history)\n",
    "        response = self._extract_tool()\n",
    "        history.append((prompt, response))\n",
    "        return response\n",
    " \n",
    " \n",
    "    def doRequest(self,payload:dict) -> str:\n",
    "        # 请求头\n",
    "        headers = {\"content-type\":\"application/json\"}\n",
    "        # json形式，参数用json\n",
    "        res = requests.post(self.url,json=payload,headers=headers)\n",
    "        return res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7a480-1311-4032-bdac-ec88e50b6abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
